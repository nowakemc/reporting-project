import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
import numpy as np
import base64
from io import BytesIO
import time
from datetime import datetime
import re
import os
import json
from collections import Counter, defaultdict

# Set page config
st.set_page_config(
    layout="wide",
    page_title="ðŸ” Enhanced File Explorer Visualization",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'file_data_loaded' not in st.session_state:
    st.session_state.file_data_loaded = False
    st.session_state.file_extensions = []
    st.session_state.creation_dates = []
    st.session_state.folder_depth = 3
    st.session_state.size_unit = "MB"
    st.session_state.selected_tab = "Folder Structure"

# Helper functions
def convert_size(size_bytes, unit="MB"):
    """Convert size from bytes to specified unit"""
    if pd.isna(size_bytes):
        return 0
        
    if unit == "Bytes":
        return size_bytes
    elif unit == "KB":
        return size_bytes / 1024
    elif unit == "MB":
        return size_bytes / (1024 * 1024)
    elif unit == "GB":
        return size_bytes / (1024 * 1024 * 1024)
    else:
        return size_bytes

def extract_extension(filename):
    """Extract file extension from filename"""
    if pd.isna(filename) or filename == '':
        return "unknown"
        
    parts = str(filename).split('.')
    if len(parts) > 1 and parts[-1]:
        return parts[-1].lower()
    return "no_extension"

def parse_datetime(dt_str):
    """Parse datetime from string with error handling"""
    if pd.isna(dt_str) or dt_str == 'nan' or dt_str == '':
        return None
        
    try:
        # Try common formats
        for fmt in ["%Y-%m-%dT%H:%M:%S.%fZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S"]:
            try:
                return datetime.strptime(dt_str, fmt)
            except ValueError:
                continue
        
        # If all fail, return None
        return None
    except:
        return None

def get_file_age_days(dt_str, reference_date=None):
    """Calculate file age in days"""
    if reference_date is None:
        reference_date = datetime.now()
        
    dt = parse_datetime(dt_str)
    if dt is None:
        return None
        
    delta = reference_date - dt
    return delta.days

def process_folder_path(path):
    """Process folder path into components"""
    if pd.isna(path) or path == '':
        return []
        
    # Convert to string if needed
    path = str(path)
    
    # Remove leading/trailing slashes and split
    clean_path = path.strip('/')
    if not clean_path:
        return []
    return clean_path.split('/')

def aggregate_by_folder(df):
    """Aggregate file data by folder"""
    # Ensure we have a valid dataframe
    if df is None or df.empty:
        st.error("No valid data to aggregate into folders")
        return pd.DataFrame({'parentPath': ['root'], 
                           'folder_size': [0], 
                           'file_count': [0], 
                           'avg_file_size': [0],
                           'deleted_count': [0]})
    
    # Prepare date columns if they exist - convert to consistent type
    date_columns = ['createTime', 'modifyTime', 'accessTime']
    for col in date_columns:
        if col in df.columns:
            # Convert all values to strings first to ensure consistent types
            df[col] = df[col].astype(str)
    
    # Group by parentPath with safe aggregations
    try:
        folder_stats = df.groupby('parentPath').agg(
            folder_size=('size', 'sum'),
            file_count=('name', 'count'),
            avg_file_size=('size', 'mean')
        ).reset_index()
        
        # Add time-based stats only if columns exist
        if 'createTime' in df.columns:
            oldest = df.groupby('parentPath')['createTime'].min().reset_index()
            newest = df.groupby('parentPath')['createTime'].max().reset_index()
            folder_stats = folder_stats.merge(oldest, on='parentPath', how='left')
            folder_stats = folder_stats.merge(newest, on='parentPath', how='left', suffixes=('', '_newest'))
            folder_stats.rename(columns={'createTime': 'oldest_file', 'createTime_newest': 'newest_file'}, inplace=True)
        
        # Add deleted count if column exists
        if 'isDeleted' in df.columns:
            deleted = df.groupby('parentPath')['isDeleted'].apply(lambda x: (x == 1).sum()).reset_index()
            folder_stats = folder_stats.merge(deleted, on='parentPath', how='left')
            folder_stats.rename(columns={'isDeleted': 'deleted_count'}, inplace=True)
            folder_stats['deleted_count'] = folder_stats['deleted_count'].fillna(0).astype(int)
        else:
            folder_stats['deleted_count'] = 0
    
    except Exception as e:
        st.error(f"Error aggregating folder data: {str(e)}")
        st.exception(e)
        # Return minimal dataframe to prevent application crash
        return pd.DataFrame({'parentPath': df['parentPath'].unique() if 'parentPath' in df.columns else ['root'], 
                           'folder_size': [0] * (len(df['parentPath'].unique()) if 'parentPath' in df.columns else 1), 
                           'file_count': [0] * (len(df['parentPath'].unique()) if 'parentPath' in df.columns else 1), 
                           'avg_file_size': [0] * (len(df['parentPath'].unique()) if 'parentPath' in df.columns else 1),
                           'deleted_count': [0] * (len(df['parentPath'].unique()) if 'parentPath' in df.columns else 1)})
    
    # Add folder depth
    try:
        folder_stats['folder_components'] = folder_stats['parentPath'].apply(process_folder_path)
        folder_stats['folder_depth'] = folder_stats['folder_components'].apply(len)
    except Exception as e:
        st.warning(f"Error calculating folder depth: {str(e)}")
        # Provide default values
        folder_stats['folder_components'] = [[]] * len(folder_stats)
        folder_stats['folder_depth'] = [0] * len(folder_stats)
    
    return folder_stats

def prepare_folder_hierarchy(folder_stats, max_depth=10):
    """Prepare folder hierarchy for visualization"""
    # Check if folder_stats is None or empty
    if folder_stats is None:
        st.error("No folder statistics available to create hierarchy")
        # Return a minimal valid DataFrame
        return pd.DataFrame({'parentPath': ['root'], 
                          'folder_size': [0], 
                          'file_count': [0],
                          'folder_depth': [1],
                          'Level 1': ['root']})
    
    # Create folder level columns
    try:
        max_actual_depth = max(folder_stats['folder_depth']) if not folder_stats.empty else 0
        for i in range(min(max_actual_depth, max_depth)):
            folder_stats[f'Level {i+1}'] = folder_stats['folder_components'].apply(
                lambda x: x[i] if len(x) > i else ""
            )
    except Exception as e:
        st.error(f"Error creating folder hierarchy: {str(e)}")
        # Create a single level as fallback
        folder_stats['Level 1'] = folder_stats['parentPath'].apply(
            lambda x: x.split('/')[-1] if x and '/' in x else x
        )
    
    return folder_stats

def render_access_patterns_tab(df):
    """Render the Access Patterns tab"""
    st.subheader("ðŸ“‚ File Access Patterns Analysis")
    
    col1, col2 = st.columns([1, 3])
    
    with col1:
        # Date range options
        if 'access_date' in df.columns:
            min_access = df['access_date'].min()
            max_access = df['access_date'].max()
            
            if pd.notna(min_access) and pd.notna(max_access):
                # Format dates for display
                st.write("**Access Date Range**")
                min_access_str = min_access.strftime("%Y-%m-%d") if hasattr(min_access, 'strftime') else "Unknown"
                max_access_str = max_access.strftime("%Y-%m-%d") if hasattr(max_access, 'strftime') else "Unknown"
                st.write(f"Earliest Access: {min_access_str}")
                st.write(f"Latest Access: {max_access_str}")
        
        # Visualization options
        viz_type = st.selectbox(
            "Visualization Type",
            ["Access Timeline", "Stale Files", "Usage Frequency", "Access vs. Modification", "Time Heatmap"]
        )
        
        # Size unit for coloring/sizing
        size_unit = st.selectbox(
            "Size Unit",
            ["Bytes", "KB", "MB", "GB"],
            index=["Bytes", "KB", "MB", "GB"].index(st.session_state.size_unit)
        )
        st.session_state.size_unit = size_unit
        
        # Specific controls for different visualizations
        if viz_type == "Stale Files":
            stale_threshold = st.slider(
                "Stale Threshold (days without access)",
                min_value=30,
                max_value=365,
                value=90
            )
        
        elif viz_type == "Time Heatmap":
            time_granularity = st.selectbox(
                "Time Granularity",
                ["Hour of Day", "Day of Week", "Month of Year"]
            )
    
    with col2:
        # Filter out rows with missing access times
        df_access = df.dropna(subset=['accessTime']).copy()
        
        if len(df_access) == 0:
            st.warning("No valid access times found in the data.")
        else:
            # Convert sizes
            df_access['size_converted'] = df_access['size'].apply(
                lambda x: convert_size(x, size_unit)
            )
            
            try:
                if viz_type == "Access Timeline":
                    # Aggregate by access date
                    df_access['access_date'] = pd.to_datetime(df_access['accessTime']).dt.date
                    timeline_data = df_access.groupby('access_date').agg(
                        file_count=('name', 'count'),
                        total_size=('size', 'sum')
                    ).reset_index()
                    
                    timeline_data['total_size_converted'] = timeline_data['total_size'].apply(
                        lambda x: convert_size(x, size_unit)
                    )
                    
                    fig = px.scatter(
                        timeline_data,
                        x='access_date',
                        y='file_count',
                        size='total_size_converted',
                        title="Files Accessed Over Time",
                        color='total_size_converted',
                        labels={
                            'access_date': 'Access Date',
                            'file_count': 'Number of Files',
                            'total_size_converted': f'Total Size ({size_unit})'
                        },
                        hover_data=['total_size_converted']
                    )
                    
                elif viz_type == "Stale Files":
                    # Calculate staleness
                    now = datetime.now()
                    df_access['days_since_access'] = (now - pd.to_datetime(df_access['accessTime'])).dt.days
                    df_access['is_stale'] = df_access['days_since_access'] > stale_threshold
                    
                    # Group by extension and staleness
                    stale_by_ext = df_access.groupby(['extension', 'is_stale']).agg(
                        file_count=('name', 'count'),
                        total_size=('size', 'sum')
                    ).reset_index()
                    
                    stale_by_ext['total_size_converted'] = stale_by_ext['total_size'].apply(
                        lambda x: convert_size(x, size_unit)
                    )
                    
                    # Get top extensions
                    top_exts = df_access['extension'].value_counts().head(10).index.tolist()
                    stale_filtered = stale_by_ext[stale_by_ext['extension'].isin(top_exts)]
                    
                    fig = px.bar(
                        stale_filtered,
                        x='extension',
                        y='file_count',
                        color='is_stale',
                        barmode='group',
                        title=f"Stale Files Analysis (>{stale_threshold} days without access)",
                        color_discrete_map={True: '#e74c3c', False: '#3498db'},
                        labels={
                            'extension': 'File Extension',
                            'file_count': 'Number of Files',
                            'is_stale': f'Stale (>{stale_threshold} days)'
                        },
                        hover_data=['total_size_converted']
                    )
                
                elif viz_type == "Usage Frequency":
                    # Calculate file age and access recency
                    if 'createTime' in df_access.columns and 'days_since_access' in df_access.columns:
                        df_access['file_age_days'] = (pd.to_datetime(df_access['accessTime']) - pd.to_datetime(df_access['createTime'])).dt.days
                        
                        # Create a usage score - higher is more frequently used relative to age
                        # A file that was accessed recently relative to its age has higher usage
                        df_access['usage_score'] = df_access.apply(
                            lambda row: 1 - (row['days_since_access'] / (row['file_age_days'] + 1))
                            if pd.notna(row.get('days_since_access')) and pd.notna(row.get('file_age_days')) else 0,
                            axis=1
                        )
                        
                        # Filter to files with positive age
                        usage_data = df_access[df_access['file_age_days'] > 0].copy()
                        
                        fig = px.scatter(
                            usage_data,
                            x='file_age_days',
                            y='days_since_access',
                            size='size_converted',
                            color='usage_score',
                            title="File Usage Frequency Analysis",
                            labels={
                                'file_age_days': 'File Age (days)',
                                'days_since_access': 'Days Since Last Access',
                                'usage_score': 'Usage Score (higher = more frequent)',
                                'size_converted': f'Size ({size_unit})'
                            },
                            color_continuous_scale='viridis',
                            hover_data=['name', 'extension', 'parentPath'],
                            opacity=0.7
                        )
                        
                        # Add reference lines
                        fig.add_hline(y=30, line_dash="dash", line_color="red", 
                                     annotation_text="30 days", annotation_position="bottom right")
                        fig.add_hline(y=90, line_dash="dash", line_color="orange", 
                                     annotation_text="90 days", annotation_position="bottom right")
                    else:
                        st.warning("Both createTime and accessTime are needed for Usage Frequency analysis.")
                        fig = None
                
                elif viz_type == "Access vs. Modification":
                    # Calculate the gap between modification and access
                    if 'modifyTime' in df_access.columns:
                        df_access['mod_date'] = pd.to_datetime(df_access['modifyTime'])
                        df_access['access_date'] = pd.to_datetime(df_access['accessTime'])
                        
                        # Calculate gap in days (access - modification)
                        # Positive = accessed after modification (normal)
                        # Negative = modified after last access (unusual)
                        # Near zero = accessed right after modification
                        df_access['access_mod_gap'] = (df_access['access_date'] - df_access['mod_date']).dt.days
                        
                        # Group by extension and calculate average gap
                        gap_by_ext = df_access.groupby('extension').agg(
                            avg_gap=('access_mod_gap', 'mean'),
                            file_count=('name', 'count'),
                            total_size=('size', 'sum')
                        ).reset_index()
                        
                        gap_by_ext['total_size_converted'] = gap_by_ext['total_size'].apply(
                            lambda x: convert_size(x, size_unit)
                        )
                        
                        # Get top extensions by file count
                        top_exts = gap_by_ext.sort_values('file_count', ascending=False).head(15)
                        
                        fig = px.bar(
                            top_exts,
                            x='extension',
                            y='avg_gap',
                            title="Average Days Between Modification and Access by Extension",
                            color='avg_gap',
                            labels={
                                'extension': 'File Extension',
                                'avg_gap': 'Avg. Days (Access - Modification)',
                                'file_count': 'Number of Files'
                            },
                            color_continuous_scale='RdBu',
                            color_continuous_midpoint=0,
                            hover_data=['file_count', 'total_size_converted'],
                            text_auto='.1f'
                        )
                    else:
                        st.warning("modifyTime is needed for Access vs. Modification analysis.")
                        fig = None
                
                else:  # Time Heatmap
                    # Extract time components based on selected granularity
                    df_access['access_date'] = pd.to_datetime(df_access['accessTime'])
                    
                    if time_granularity == "Hour of Day":
                        df_access['time_component'] = df_access['access_date'].dt.hour
                        time_labels = {i: f"{i}:00" for i in range(24)}
                        title = "File Access by Hour of Day"
                        xlabel = "Hour (24-hour format)"
                    elif time_granularity == "Day of Week":
                        df_access['time_component'] = df_access['access_date'].dt.dayofweek
                        time_labels = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 
                                     3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}
                        title = "File Access by Day of Week"
                        xlabel = "Day of Week" 
                    else:  # Month of Year
                        df_access['time_component'] = df_access['access_date'].dt.month
                        time_labels = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',
                                     7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}
                        title = "File Access by Month of Year"
                        xlabel = "Month"
                    
                    # Aggregate by time component
                    time_counts = df_access.groupby('time_component').agg(
                        file_count=('name', 'count'),
                        total_size=('size', 'sum')
                    ).reset_index()
                    
                    # Add labels
                    time_counts['time_label'] = time_counts['time_component'].map(time_labels)
                    
                    # Convert size
                    time_counts['total_size_converted'] = time_counts['total_size'].apply(
                        lambda x: convert_size(x, size_unit)
                    )
                    
                    # Create a heatmap-like bar chart
                    fig = px.bar(
                        time_counts,
                        x='time_component',
                        y='file_count',
                        title=title,
                        color='file_count',
                        labels={
                            'time_component': xlabel,
                            'file_count': 'Number of Files',
                            'time_label': 'Time Period'
                        },
                        color_continuous_scale='Viridis',
                        hover_data=['time_label', 'total_size_converted'],
                        text='time_label'
                    )
                    
                    # Format x-axis to show all time periods
                    if time_granularity == "Hour of Day":
                        fig.update_xaxes(tickvals=list(range(24)))
                    elif time_granularity == "Day of Week":
                        fig.update_xaxes(tickvals=list(range(7)))
                    else:  # Month of Year
                        fig.update_xaxes(tickvals=list(range(1, 13)))
                
                if fig:
                    fig.update_layout(margin=dict(t=50, l=25, r=25, b=25))
                    st.plotly_chart(fig, use_container_width=True)
            
            except Exception as e:
                st.error(f"Error creating visualization: {str(e)}")
                st.exception(e)
    
    # Additional access statistics
    st.subheader("ðŸ“Š Access Statistics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**General Access Statistics**")
        
        df_access = df.dropna(subset=['accessTime']).copy()
        
        if len(df_access) > 0:
            # Calculate days since last access
            df_access['access_date'] = pd.to_datetime(df_access['accessTime'])
            now = datetime.now()
            df_access['days_since_access'] = (now - df_access['access_date']).dt.days
            
            median_access_days = df_access['days_since_access'].median()
            mean_access_days = df_access['days_since_access'].mean()
            
            st.write(f"Files with access data: {len(df_access):,}")
            st.write(f"Median days since last access: {median_access_days:.1f} days")
            st.write(f"Mean days since last access: {mean_access_days:.1f} days")
            
            # Calculate access recency buckets
            access_buckets = [
                (0, 7, "Last 7 days"),
                (7, 30, "Last 30 days"),
                (30, 90, "Last 90 days"),
                (90, 180, "Last 180 days"),
                (180, 365, "Last year"),
                (365, float('inf'), "Over a year")
            ]
            
            bucket_counts = []
            for min_days, max_days, label in access_buckets:
                count = df_access[(df_access['days_since_access'] >= min_days) & 
                                 (df_access['days_since_access'] < max_days)].shape[0]
                bucket_counts.append({
                    'bucket': label,
                    'count': count,
                    'percentage': count / len(df_access) * 100
                })
            
            bucket_df = pd.DataFrame(bucket_counts)
            st.write("**Access Recency Distribution**")
            st.dataframe(bucket_df)
        else:
            st.write("No files with valid access times found.")
    
    with col2:
        if len(df_access) > 0:
            # Create a histogram of days since last access
            hist_fig = px.histogram(
                df_access,
                x='days_since_access',
                nbins=50,
                title="Distribution of Days Since Last Access",
                labels={'days_since_access': 'Days Since Last Access'}
            )
            
            # Add reference lines
            hist_fig.add_vline(x=30, line_dash="dash", line_color="yellow", 
                             annotation_text="30 days", annotation_position="top right")
            hist_fig.add_vline(x=90, line_dash="dash", line_color="orange", 
                             annotation_text="90 days", annotation_position="top right")
            hist_fig.add_vline(x=365, line_dash="dash", line_color="red", 
                             annotation_text="1 year", annotation_position="top right")
            
            st.plotly_chart(hist_fig, use_container_width=True)
            
            # If both access and create time are available, show creation-to-first-access stats
            if 'createTime' in df.columns:
                st.write("**Creation to Access Patterns**")
                
                # Calculate time from creation to access
                df_both_times = df.dropna(subset=['accessTime', 'createTime']).copy()
                
                if len(df_both_times) > 0:
                    df_both_times['create_date'] = pd.to_datetime(df_both_times['createTime']) 
                    df_both_times['access_date'] = pd.to_datetime(df_both_times['accessTime'])
                    
                    # Only include files where access is after creation
                    df_both_times = df_both_times[df_both_times['access_date'] >= df_both_times['create_date']]
                    
                    if len(df_both_times) > 0:
                        df_both_times['days_to_access'] = (df_both_times['access_date'] - 
                                                         df_both_times['create_date']).dt.days
                        
                        median_days_to_access = df_both_times['days_to_access'].median()
                        
                        st.write(f"Median days from creation to access: {median_days_to_access:.1f} days")
                        
                        # Count files accessed on the same day as creation
                        same_day = (df_both_times['days_to_access'] == 0).sum()
                        same_day_pct = same_day / len(df_both_times) * 100
                        
                        st.write(f"Files accessed same day as creation: {same_day:,} ({same_day_pct:.1f}%)")
                        
                        # Count files never accessed after creation day
                        never_accessed = len(df) - len(df_both_times)
                        never_accessed_pct = never_accessed / len(df) * 100
                        
                        st.write(f"Files never accessed after creation: {never_accessed:,} ({never_accessed_pct:.1f}%)")

def render_stale_files_dashboard(df):
    """Render a specialized dashboard for stale files analysis"""
    st.subheader("ðŸ•’ Stale Files Dashboard")
    
    # Ensure we have the necessary data
    if 'accessTime' not in df.columns or 'createTime' not in df.columns:
        st.warning("This dashboard requires both accessTime and createTime data.")
        return
    
    # Process the data
    df_stale = df.copy()
    
    # Convert date fields to datetime
    df_stale['access_date'] = pd.to_datetime(df_stale['accessTime'])
    df_stale['create_date'] = pd.to_datetime(df_stale['createTime'])
    
    # Calculate staleness metrics
    now = datetime.now()
    df_stale['days_since_access'] = (now - df_stale['access_date']).dt.days
    df_stale['days_since_creation'] = (now - df_stale['create_date']).dt.days
    df_stale['days_unused_ratio'] = df_stale['days_since_access'] / df_stale['days_since_creation']
    
    # Define staleness thresholds
    thresholds = {
        'fresh': 30,       # Less than 30 days
        'recent': 90,      # 30-90 days
        'aging': 180,      # 90-180 days
        'stale': 365,      # 180-365 days
        'archive': float('inf')  # Over 365 days
    }
    
    # Create staleness category
    def get_staleness_category(days):
        if pd.isna(days):
            return "unknown"
        elif days < thresholds['fresh']:
            return "fresh"
        elif days < thresholds['recent']:
            return "recent"
        elif days < thresholds['aging']:
            return "aging"
        elif days < thresholds['stale']:
            return "stale"
        else:
            return "archive"
    
    df_stale['staleness'] = df_stale['days_since_access'].apply(get_staleness_category)
    
    # Layout
    col1, col2 = st.columns([1, 2])
    
    with col1:
        # Staleness threshold controls
        st.write("**Staleness Thresholds (days)**")
        custom_fresh = st.slider("Fresh cutoff", 1, 60, 30)
        custom_recent = st.slider("Recent cutoff", custom_fresh+1, 120, 90)
        custom_aging = st.slider("Aging cutoff", custom_recent+1, 240, 180)
        custom_stale = st.slider("Stale cutoff", custom_aging+1, 500, 365)
        
        # Update thresholds
        thresholds = {
            'fresh': custom_fresh,
            'recent': custom_recent,
            'aging': custom_aging,
            'stale': custom_stale,
            'archive': float('inf')
        }
        
        # Recalculate staleness with new thresholds
        df_stale['staleness'] = df_stale['days_since_access'].apply(get_staleness_category)
        
        # Size unit selection
        size_unit = st.selectbox(
            "Size Unit",
            ["Bytes", "KB", "MB", "GB"],
            index=2  # Default to MB
        )
        
        # Storage reclamation target
        st.write("**Storage Reclamation Target**")
        reclamation_options = ["Archive Files", "Stale Files", "Aging Files"]
        reclamation_selections = st.multiselect("File categories to consider", 
                                             reclamation_options,
                                             default=["Archive Files", "Stale Files"])
        
        # Convert selected categories to staleness values
        reclamation_categories = []
        if "Archive Files" in reclamation_selections:
            reclamation_categories.append("archive")
        if "Stale Files" in reclamation_selections:
            reclamation_categories.append("stale")
        if "Aging Files" in reclamation_selections:
            reclamation_categories.append("aging")
        
        # Calculate potential space savings
        if reclamation_categories:
            files_to_reclaim = df_stale[df_stale['staleness'].isin(reclamation_categories)]
            reclamation_size = files_to_reclaim['size'].sum()
            reclamation_count = len(files_to_reclaim)
            
            total_size = df_stale['size'].sum()
            reclamation_pct = (reclamation_size / total_size * 100) if total_size > 0 else 0
            
            st.metric(
                "Potential Space Savings", 
                f"{convert_size(reclamation_size, size_unit):.2f} {size_unit}",
                f"{reclamation_pct:.1f}% of total"
            )
            
            st.metric(
                "Files Affected",
                f"{reclamation_count:,}",
                f"{reclamation_count/len(df_stale)*100:.1f}% of total"
            )
    
    with col2:
        # Visualizations
        
        # 1. Staleness distribution pie chart
        staleness_counts = df_stale['staleness'].value_counts().reset_index()
        staleness_counts.columns = ['Category', 'Count']
        
        # Define order and colors
        category_order = ['fresh', 'recent', 'aging', 'stale', 'archive', 'unknown']
        color_map = {
            'fresh': '#2ecc71',    # Green
            'recent': '#3498db',   # Blue
            'aging': '#f1c40f',    # Yellow
            'stale': '#e67e22',    # Orange
            'archive': '#e74c3c',  # Red
            'unknown': '#95a5a6'   # Gray
        }
        
        # Add readable labels
        label_map = {
            'fresh': f'Fresh (<{thresholds["fresh"]} days)',
            'recent': f'Recent ({thresholds["fresh"]}-{thresholds["recent"]} days)',
            'aging': f'Aging ({thresholds["recent"]}-{thresholds["aging"]} days)',
            'stale': f'Stale ({thresholds["aging"]}-{thresholds["stale"]} days)',
            'archive': f'Archive (>{thresholds["stale"]} days)',
            'unknown': 'Unknown'
        }
        
        staleness_counts['Label'] = staleness_counts['Category'].map(label_map)
        
        # Sort by predefined order
        staleness_counts['sort_order'] = staleness_counts['Category'].apply(
            lambda x: category_order.index(x) if x in category_order else len(category_order)
        )
        staleness_counts = staleness_counts.sort_values('sort_order')
        
        fig1 = px.pie(
            staleness_counts,
            names='Label',
            values='Count',
            title="Distribution of File Staleness",
            color='Category',
            color_discrete_map=color_map
        )
        fig1.update_traces(textposition='inside', textinfo='percent+label')
        
        st.plotly_chart(fig1, use_container_width=True)
        
        # 2. Size by staleness
        staleness_size = df_stale.groupby('staleness').agg(
            total_size=('size', 'sum'),
            file_count=('name', 'count')
        ).reset_index()
        
        staleness_size['size_converted'] = staleness_size['total_size'].apply(
            lambda x: convert_size(x, size_unit)
        )
        
        # Add readable labels
        staleness_size['Label'] = staleness_size['staleness'].map(label_map)
        
        # Sort by predefined order
        staleness_size['sort_order'] = staleness_size['staleness'].apply(
            lambda x: category_order.index(x) if x in category_order else len(category_order)
        )
        staleness_size = staleness_size.sort_values('sort_order')
        
        fig2 = px.bar(
            staleness_size,
            x='Label',
            y='size_converted',
            title=f"Total Size by Staleness Category ({size_unit})",
            color='staleness',
            color_discrete_map=color_map,
            text_auto='.2f',
            hover_data=['file_count']
        )
        
        st.plotly_chart(fig2, use_container_width=True)
    
    # Extension analysis for stale files
    st.subheader("File Types Contributing to Staleness")
    
    # Get stale and archive files
    inactive_files = df_stale[df_stale['staleness'].isin(['stale', 'archive'])]
    
    if len(inactive_files) > 0:
        # Group by extension
        ext_staleness = inactive_files.groupby('extension').agg(
            file_count=('name', 'count'),
            total_size=('size', 'sum'),
            avg_days_unused=('days_since_access', 'mean')
        ).reset_index()
        
        ext_staleness['size_converted'] = ext_staleness['total_size'].apply(
            lambda x: convert_size(x, size_unit)
        )
        
        # Sort by total size
        ext_staleness = ext_staleness.sort_values('total_size', ascending=False).head(15)
        
        # Create horizontal bar chart
        fig3 = px.bar(
            ext_staleness,
            y='extension',
            x='size_converted',
            orientation='h',
            title=f"Top 15 Extensions Contributing to Stale Storage ({size_unit})",
            color='avg_days_unused',
            text_auto='.2f',
            labels={
                'extension': 'File Extension',
                'size_converted': f'Total Size ({size_unit})',
                'avg_days_unused': 'Avg Days Since Last Access'
            },
            color_continuous_scale='Reds',
            hover_data=['file_count', 'avg_days_unused']
        )
        fig3.update_layout(yaxis={'categoryorder': 'total ascending'})
        
        st.plotly_chart(fig3, use_container_width=True)
        
        # Display top folders with stale content
        st.subheader("Top Folders with Stale Content")
        
        folder_staleness = inactive_files.groupby('parentPath').agg(
            file_count=('name', 'count'),
            total_size=('size', 'sum'),
            avg_days_unused=('days_since_access', 'mean')
        ).reset_index()
        
        folder_staleness['size_converted'] = folder_staleness['total_size'].apply(
            lambda x: convert_size(x, size_unit)
        )
        
        # Extract folder name for display
        folder_staleness['folder_name'] = folder_staleness['parentPath'].apply(
            lambda x: x.split('/')[-1] if x and '/' in x else x
        )
        
        # Sort by total size and get top 10
        top_stale_folders = folder_staleness.sort_values('total_size', ascending=False).head(10)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Create a table view
            st.write("**Top Folders by Stale Storage**")
            display_data = top_stale_folders[['folder_name', 'parentPath', 'file_count', 'size_converted', 'avg_days_unused']]
            display_data.columns = ['Folder', 'Path', 'Files', f'Size ({size_unit})', 'Avg Days Unused']
            st.dataframe(display_data)
        
        with col2:
            # Create a treemap of top folders
            fig4 = px.treemap(
                top_stale_folders,
                path=['folder_name'],
                values='total_size',
                color='avg_days_unused',
                hover_data=['file_count', 'size_converted'],
                color_continuous_scale='Reds',
                title="Top Folders with Stale Content"
            )
            
            st.plotly_chart(fig4, use_container_width=True)
    else:
        st.info("No stale or archive files found based on current thresholds.")

def render_access_modification_comparison(df):
    """Render a comparison of access and modification patterns"""
    st.subheader("ðŸ“ˆ Access vs. Modification Timeline Comparison")
    
    # Check if we have the necessary data
    if 'accessTime' not in df.columns or 'modifyTime' not in df.columns:
        st.warning("This visualization requires both accessTime and modifyTime data.")
        return
    
    # Prepare the data
    df_timeline = df.copy()
    
    # Convert timestamps to datetime
    df_timeline['access_date'] = pd.to_datetime(df_timeline['accessTime'])
    df_timeline['modify_date'] = pd.to_datetime(df_timeline['modifyTime'])
    
    # Extract just the date portion for grouping
    df_timeline['access_day'] = df_timeline['access_date'].dt.date
    df_timeline['modify_day'] = df_timeline['modify_date'].dt.date
    
    # Group by date and count access and modification events
    access_counts = df_timeline.groupby('access_day').size().reset_index(name='access_count')
    modify_counts = df_timeline.groupby('modify_day').size().reset_index(name='modify_count')
    
    # Merge the two datasets
    timeline_data = pd.merge(
        access_counts, 
        modify_counts, 
        left_on='access_day', 
        right_on='modify_day', 
        how='outer'
    ).fillna(0)
    
    # Clean up the columns
    timeline_data['date'] = timeline_data['access_day'].combine_first(timeline_data['modify_day'])
    timeline_data = timeline_data[['date', 'access_count', 'modify_count']]
    
    # Sort by date
    timeline_data = timeline_data.sort_values('date')
    
    # Layout
    col1, col2 = st.columns([1, 3])
    
    with col1:
        # Time range options
        if not timeline_data.empty:
            min_date = timeline_data['date'].min()
            max_date = timeline_data['date'].max()
            
            # Date range selection
            date_range = st.date_input(
                "Select Date Range",
                value=(min_date, max_date),
                min_value=min_date,
                max_value=max_date
            )
            
            # Handle single date selection
            if isinstance(date_range, tuple) and len(date_range) == 2:
                start_date, end_date = date_range
            else:
                start_date = date_range
                end_date = date_range
            
            # Analysis options
            analysis_type = st.selectbox(
                "Analysis Type",
                ["Counts Over Time", "Cumulative Growth", "Rolling Average", "Ratio Analysis"]
            )
            
            # For rolling average
            if analysis_type == "Rolling Average":
                window_size = st.slider(
                    "Window Size (days)",
                    min_value=3,
                    max_value=30,
                    value=7
                )
            
            # Aggregation options
            if max_date - min_date > pd.Timedelta(days=90):
                aggregation = st.selectbox(
                    "Time Aggregation",
                    ["Daily", "Weekly", "Monthly"]
                )
            else:
                aggregation = st.selectbox(
                    "Time Aggregation",
                    ["Daily", "Weekly"]
                )
    
    with col2:
        # Filter by date range
        filtered_data = timeline_data[
            (timeline_data['date'] >= start_date) & 
            (timeline_data['date'] <= end_date)
        ].copy()
        
        if filtered_data.empty:
            st.warning("No data available for the selected date range.")
            return
        
        # Apply time aggregation
        if aggregation == "Weekly":
            # Convert date to pandas datetime for proper resampling
            filtered_data['date'] = pd.to_datetime(filtered_data['date'])
            filtered_data = filtered_data.set_index('date')
            
            # Resample to weekly data
            filtered_data = filtered_data.resample('W').sum().reset_index()
        elif aggregation == "Monthly":
            # Convert date to pandas datetime for proper resampling
            filtered_data['date'] = pd.to_datetime(filtered_data['date'])
            filtered_data = filtered_data.set_index('date')
            
            # Resample to monthly data
            filtered_data = filtered_data.resample('M').sum().reset_index()
        
        # Apply analysis type
        if analysis_type == "Counts Over Time":
            # Prepare the figure
            fig = go.Figure()
            
            # Add access count line
            fig.add_trace(go.Scatter(
                x=filtered_data['date'],
                y=filtered_data['access_count'],
                mode='lines+markers',
                name='Access Events',
                line=dict(color='#3498db', width=2),
                marker=dict(size=6)
            ))
            
            # Add modification count line
            fig.add_trace(go.Scatter(
                x=filtered_data['date'],
                y=filtered_data['modify_count'],
                mode='lines+markers',
                name='Modification Events',
                line=dict(color='#e74c3c', width=2),
                marker=dict(size=6)
            ))
            
            # Update layout
            fig.update_layout(
                title="File Access vs. Modification Events Over Time",
                xaxis_title="Date",
                yaxis_title="Number of Events",
                hovermode="x unified"
            )
        
        elif analysis_type == "Cumulative Growth":
            # Calculate cumulative sums
            filtered_data['cumulative_access'] = filtered_data['access_count'].cumsum()
            filtered_data['cumulative_modify'] = filtered_data['modify_count'].cumsum()
            
            # Prepare the figure
            fig = go.Figure()
            
            # Add cumulative access line
            fig.add_trace(go.Scatter(
                x=filtered_data['date'],
                y=filtered_data['cumulative_access'],
                mode='lines',
                name='Cumulative Access',
                line=dict(color='#3498db', width=3),
                fill='tozeroy',
                fillcolor='rgba(52, 152, 219, 0.2)'
            ))
            
            # Add cumulative modification line
            fig.add_trace(go.Scatter(
                x=filtered_data['date'],
                y=filtered_data['cumulative_modify'],
                mode='lines',
                name='Cumulative Modifications',
                line=dict(color='#e74c3c', width=3),
                fill='tozeroy',
                fillcolor='rgba(231, 76, 60, 0.2)'
            ))
            
            # Update layout
            fig.update_layout(
                title="Cumulative Growth of File Access and Modification Events",
                xaxis_title="Date",
                yaxis_title="Cumulative Number of Events",
                hovermode="x unified"
            )
        
        elif analysis_type == "Rolling Average":
            # Calculate rolling averages
            # Convert date to pandas datetime for proper rolling
            filtered_data['date'] = pd.to_datetime(filtered_data['date'])
            filtered_data = filtered_data.set_index('date')
            
            # Calculate rolling averages
            filtered_data['access_rolling'] = filtered_data['access_count'].rolling(window=window_size).mean()
            filtered_data['modify_rolling'] = filtered_data['modify_count'].rolling(window=window_size).mean()
            
            # Reset index
            filtered_data = filtered_data.reset_index()
            
            # Prepare the figure
            fig = go.Figure()
            
            # Add access rolling average line
            fig.add_trace(go.Scatter(
                x=filtered_data['date'],
                y=filtered_data['access_rolling'],
                mode='lines',
                name=f'{window_size}-Day Rolling Avg (Access)',
                line=dict(color='#3498db', width=3)
            ))
            
            # Add modification rolling average line
            fig.add_trace(go.Scatter(
                x=filtered_data['date'],
                y=filtered_data['modify_rolling'],
                mode='lines',
                name=f'{window_size}-Day Rolling Avg (Modify)',
                line=dict(color='#e74c3c', width=3)
            ))
            
            # Update layout
            fig.update_layout(
                title=f"{window_size}-Day Rolling Average of Access and Modification Events",
                xaxis_title="Date",
                yaxis_title="Average Number of Events",
                hovermode="x unified"
            )
        
        else:  # Ratio Analysis
            # Calculate access to modification ratio
            # Add a small epsilon to avoid division by zero
            epsilon = 1e-10
            filtered_data['access_modify_ratio'] = filtered_data['access_count'] / (filtered_data['modify_count'] + epsilon)
            
            # Create a color scale based on ratio (1.0 = neutral)
            filtered_data['ratio_color'] = filtered_data['access_modify_ratio'].apply(
                lambda x: 'rgba(52, 152, 219, 0.7)' if x > 1 else 'rgba(231, 76, 60, 0.7)'
            )
            
            filtered_data['ratio_intensity'] = filtered_data['access_modify_ratio'].apply(
                lambda x: min(abs(np.log10(x + epsilon)), 1.0)
            )
            
            # Prepare the figure
            fig = go.Figure()
            
            # Add ratio bars
            fig.add_trace(go.Bar(
                x=filtered_data['date'],
                y=filtered_data['access_modify_ratio'],
                marker_color=filtered_data['ratio_color'],
                name='Access:Modify Ratio',
                hovertemplate='Date: %{x}<br>Ratio: %{y:.2f}<br>Access: %{customdata[0]}<br>Modify: %{customdata[1]}',
                customdata=filtered_data[['access_count', 'modify_count']]
            ))
            
            # Add reference line at ratio = 1
            fig.add_hline(
                y=1.0,
                line_dash="dash",
                line_color="black",
                annotation_text="Equal Access & Modification",
                annotation_position="bottom right"
            )
            
            # Update layout
            fig.update_layout(
                title="Ratio of Access Events to Modification Events",
                xaxis_title="Date",
                yaxis_title="Access:Modify Ratio",
                hovermode="closest"
            )
            
            # Use logarithmic scale for better visualization
            fig.update_yaxes(type="log")
        
        # Display the figure
        st.plotly_chart(fig, use_container_width=True)
    
    # Activity pattern analysis
    st.subheader("Activity Pattern Analysis")
    
    # Analyze when files are being accessed vs. modified
    col1, col2 = st.columns(2)
    
    with col1:
        # Analyze by hour of day
        df_timeline['access_hour'] = df_timeline['access_date'].dt.hour
        df_timeline['modify_hour'] = df_timeline['modify_date'].dt.hour
        
        # Group by hour and count events
        access_hour_counts = df_timeline.groupby('access_hour').size().reset_index(name='access_count')
        modify_hour_counts = df_timeline.groupby('modify_hour').size().reset_index(name='modify_count')
        
        # Merge the hour data
        hour_data = pd.merge(
            access_hour_counts, 
            modify_hour_counts, 
            left_on='access_hour', 
            right_on='modify_hour', 
            how='outer'
        ).fillna(0)
        
        # Clean up columns
        hour_data['hour'] = hour_data['access_hour'].combine_first(hour_data['modify_hour']).astype(int)
        hour_data = hour_data[['hour', 'access_count', 'modify_count']]
        hour_data = hour_data.sort_values('hour')
        
        # Create hour labels
        hour_data['hour_label'] = hour_data['hour'].apply(lambda h: f"{h:02d}:00")
        
        # Create hour figure
        hour_fig = go.Figure()
        
        hour_fig.add_trace(go.Scatter(
            x=hour_data['hour'],
            y=hour_data['access_count'],
            mode='lines+markers',
            name='Access Events',
            line=dict(color='#3498db', width=2)
        ))
        
        hour_fig.add_trace(go.Scatter(
            x=hour_data['hour'],
            y=hour_data['modify_count'],
            mode='lines+markers',
            name='Modification Events',
            line=dict(color='#e74c3c', width=2)
        ))
        
        # Update layout
        hour_fig.update_layout(
            title="Activity by Hour of Day",
            xaxis_title="Hour (24-hour format)",
            yaxis_title="Number of Events"
        )
        
        # Format x-axis to show all hours
        hour_fig.update_xaxes(
            tickvals=list(range(24)),
            ticktext=[f"{h:02d}:00" for h in range(24)]
        )
        
        st.plotly_chart(hour_fig, use_container_width=True)
    
    with col2:
        # Analyze by day of week
        df_timeline['access_dow'] = df_timeline['access_date'].dt.dayofweek
        df_timeline['modify_dow'] = df_timeline['modify_date'].dt.dayofweek
        
        # Group by day of week and count events
        access_dow_counts = df_timeline.groupby('access_dow').size().reset_index(name='access_count')
        modify_dow_counts = df_timeline.groupby('modify_dow').size().reset_index(name='modify_count')
        
        # Merge the day of week data
        dow_data = pd.merge(
            access_dow_counts, 
            modify_dow_counts, 
            left_on='access_dow', 
            right_on='modify_dow', 
            how='outer'
        ).fillna(0)
        
        # Clean up columns
        dow_data['dow'] = dow_data['access_dow'].combine_first(dow_data['modify_dow']).astype(int)
        dow_data = dow_data[['dow', 'access_count', 'modify_count']]
        dow_data = dow_data.sort_values('dow')
        
        # Create day of week labels
        dow_labels = {
            0: 'Monday',
            1: 'Tuesday',
            2: 'Wednesday',
            3: 'Thursday',
            4: 'Friday',
            5: 'Saturday',
            6: 'Sunday'
        }
        dow_data['dow_label'] = dow_data['dow'].map(dow_labels)
        
        # Create day of week figure
        dow_fig = go.Figure()
        
        dow_fig.add_trace(go.Bar(
            x=dow_data['dow_label'],
            y=dow_data['access_count'],
            name='Access Events',
            marker_color='#3498db'
        ))
        
        dow_fig.add_trace(go.Bar(
            x=dow_data['dow_label'],
            y=dow_data['modify_count'],
            name='Modification Events',
            marker_color='#e74c3c'
        ))
        
        # Update layout
        dow_fig.update_layout(
            title="Activity by Day of Week",
            xaxis_title="Day",
            yaxis_title="Number of Events",
            barmode='group'
        )
        
        # Format x-axis to show all days in correct order
        dow_fig.update_xaxes(
            categoryorder='array',
            categoryarray=[dow_labels[i] for i in range(7)]
        )
        
        st.plotly_chart(dow_fig, use_container_width=True)

# App title and description
st.title("ðŸ” Enhanced File Explorer Visualization")
st.markdown("Upload your file data (CSV or JSON) to analyze and visualize file distribution.")

# Initialize empty DataFrame in case of early errors
df = pd.DataFrame()
folder_stats = pd.DataFrame()
folder_hierarchy = pd.DataFrame()

# File uploader
uploaded_file = st.sidebar.file_uploader("Upload your file data", type=["csv", "json"])

if uploaded_file:
    try:
        # Load data and provide fallback in case of errors
        with st.spinner("Processing file data..."):
            start_time = time.time()
            try:
                # Check file extension to determine how to load the data
                file_extension = uploaded_file.name.split('.')[-1].lower()
                
                if file_extension == 'csv':
                    df = pd.read_csv(uploaded_file)
                    st.sidebar.success(f"CSV loaded with {len(df)} rows and {len(df.columns)} columns")
                else:  # JSON
                    # Load JSON data
                    json_data = json.load(uploaded_file)
                    
                    # Handle different JSON structures
                    if isinstance(json_data, list):
                        # JSON is already a list of records
                        df = pd.DataFrame(json_data)
                    elif isinstance(json_data, dict):
                        # Check if it's a JSON with records under a key
                        if 'files' in json_data:
                            df = pd.DataFrame(json_data['files'])
                        elif 'records' in json_data:
                            df = pd.DataFrame(json_data['records'])
                        elif 'data' in json_data:
                            df = pd.DataFrame(json_data['data'])
                        else:
                            # Try to convert flat JSON to DataFrame
                            df = pd.DataFrame([json_data])
                    
                    st.sidebar.success(f"JSON loaded with {len(df)} rows and {len(df.columns)} columns")
            except Exception as e:
                st.error(f"Error reading file: {str(e)}")
                st.exception(e)
                # Create a minimal DataFrame to allow the app to continue
                df = pd.DataFrame({
                    'name': ['example.txt'],
                    'parentPath': ['root'],
                    'size': [0]
                })
                st.warning("Created a minimal dataset to allow visualization")
            
            # Check required columns
            required_columns = {'name', 'parentPath', 'size'}
            if not required_columns.issubset(df.columns):
                missing = required_columns - set(df.columns)
                st.error(f"Missing required columns: {', '.join(missing)}")
                
                # Add missing columns with default values to allow visualization
                for col in missing:
                    if col == 'name':
                        df['name'] = [f"file_{i}" for i in range(len(df))]
                    elif col == 'parentPath':
                        df['parentPath'] = 'root'
                    elif col == 'size':
                        df['size'] = 0
                
                st.warning("Added default values for missing columns to allow basic visualization")
            
            # Clean data and ensure all columns have appropriate types
            # Convert size to numeric, default to 0 for any values that can't be converted
            df['size'] = pd.to_numeric(df['size'], errors='coerce').fillna(0)
            
            # Make sure all text columns are actually text
            text_columns = ['name', 'parentPath']
            for col in text_columns:
                if col in df.columns:
                    df[col] = df[col].astype(str)
            
            # Handle nullable columns
            for col in ['isDeleted', 'dupKey', 'classifications']:
                if col in df.columns:
                    # Fill missing values appropriately
                    if col == 'isDeleted':
                        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
                    elif col == 'classifications':
                        df[col] = df[col].fillna('')
                    else:
                        df[col] = df[col].fillna('')
            
            # Extract useful features
            df['extension'] = df['name'].apply(extract_extension)
            
            # Parse dates if available
            if 'createTime' in df.columns:
                # First ensure the column is string type
                df['createTime'] = df['createTime'].astype(str)
                df['creation_date'] = df['createTime'].apply(parse_datetime)
                # Only calculate age for valid dates
                df['age_days'] = df.apply(
                    lambda row: get_file_age_days(row['createTime']) if pd.notna(row.get('creation_date')) else None, 
                    axis=1
                )
            
            if 'modifyTime' in df.columns:
                # First ensure the column is string type
                df['modifyTime'] = df['modifyTime'].astype(str)
                df['modification_date'] = df['modifyTime'].apply(parse_datetime)
                # Only calculate days since modified for valid dates
                df['days_since_modified'] = df.apply(
                    lambda row: get_file_age_days(row['modifyTime']) if pd.notna(row.get('modification_date')) else None,
                    axis=1
                )
            
            # Handle accessTime if available
            if 'accessTime' in df.columns:
                # Check if it's numeric (unix timestamp) or string
                if pd.api.types.is_numeric_dtype(df['accessTime']):
                    # Convert unix timestamp to datetime
                    df['accessTime'] = pd.to_datetime(df['accessTime'], unit='s')
                else:
                    # First ensure the column is string type
                    df['accessTime'] = df['accessTime'].astype(str)
                    df['access_date'] = df['accessTime'].apply(parse_datetime)
                
                # Calculate days since access for valid dates
                df['days_since_access'] = df.apply(
                    lambda row: get_file_age_days(row['accessTime']) if pd.notna(row.get('access_date')) else None,
                    axis=1
                )
            
            # Get unique file extensions
            extensions = df['extension'].value_counts().to_dict() if 'extension' in df.columns else {}
            st.session_state.file_extensions = extensions
            
            # Aggregate by folder
            try:
                folder_stats = aggregate_by_folder(df)
                if folder_stats is None or folder_stats.empty:
                    st.error("Failed to create folder statistics. Creating a minimal dataset for visualization.")
                    # Create minimal folder stats for the app to continue
                    folder_stats = pd.DataFrame({
                        'parentPath': ['root'],
                        'folder_size': [df['size'].sum()],
                        'file_count': [len(df)],
                        'avg_file_size': [df['size'].mean()],
                        'deleted_count': [0]
                    })
                    folder_stats['folder_components'] = [['root']]
                    folder_stats['folder_depth'] = [1]
                
                folder_hierarchy = prepare_folder_hierarchy(folder_stats)
                if folder_hierarchy is None:
                    st.error("Failed to create folder hierarchy. Creating a minimal dataset for visualization.")
                    # Create minimal hierarchy for the app to continue
                    folder_hierarchy = pd.DataFrame({
                        'parentPath': ['root'],
                        'folder_size': [df['size'].sum()],
                        'file_count': [len(df)],
                        'folder_depth': [1],
                        'Level 1': ['root']
                    })
            except Exception as e:
                st.error(f"Error processing folder structure: {str(e)}")
                st.exception(e)
                # Create minimal datasets to allow the app to continue
                folder_stats = pd.DataFrame({
                    'parentPath': ['root'],
                    'folder_size': [df['size'].sum()],
                    'file_count': [len(df)],
                    'avg_file_size': [df['size'].mean()],
                    'deleted_count': [0],
                    'folder_components': [['root']],
                    'folder_depth': [1]
                })
                folder_hierarchy = pd.DataFrame({
                    'parentPath': ['root'],
                    'folder_size': [df['size'].sum()],
                    'file_count': [len(df)],
                    'folder_depth': [1],
                    'Level 1': ['root']
                })
            
            # Set session state
            st.session_state.file_data = df
            st.session_state.folder_stats = folder_stats
            st.session_state.folder_hierarchy = folder_hierarchy
            st.session_state.file_data_loaded = True
            
            processing_time = time.time() - start_time
            st.sidebar.success(f"Data loaded successfully! ({processing_time:.2f}s)")
            st.sidebar.info(f"Found {len(df)} files across {len(folder_stats)} folders")
    
    except Exception as e:
        st.error(f"Error processing file: {str(e)}")
        st.exception(e)
        st.stop()

# Main visualization area
if st.session_state.file_data_loaded:
    df = st.session_state.file_data
    folder_stats = st.session_state.folder_stats
    folder_hierarchy = st.session_state.folder_hierarchy
    
    # Create tabs for different visualizations
    tabs = ["Folder Structure", "File Types", "Size Analysis", "Age Analysis"]
    if "accessTime" in df.columns:
        tabs.append("Access Patterns")
    if "classifications" in df.columns:
        tabs.append("Classifications")
    if "isDeleted" in df.columns and df["isDeleted"].sum() > 0:
        tabs.append("Deleted Files")
    if "dupKey" in df.columns:
        tabs.append("Duplicate Analysis")
    
    selected_tab = st.radio("Select Analysis View:", tabs, horizontal=True, 
                            index=tabs.index(st.session_state.selected_tab) if st.session_state.selected_tab in tabs else 0)
    st.session_state.selected_tab = selected_tab
    
    # Display visualization based on selected tab
    if selected_tab == "Folder Structure":
        st.subheader("ðŸ“ Folder Structure Analysis")
        
        col1, col2 = st.columns([1, 3])
        
        with col1:
            # Size unit selection
            size_unit = st.selectbox(
                "Size Unit",
                ["Bytes", "KB", "MB", "GB"],
                index=["Bytes", "KB", "MB", "GB"].index(st.session_state.size_unit)
            )
            st.session_state.size_unit = size_unit
            
            # Folder depth control
            max_depth = min(max(folder_stats['folder_depth']), 10)
            folder_depth = st.slider(
                "Max Folder Depth to Display",
                min_value=1,
                max_value=max_depth,
                value=min(3, max_depth)
            )
            st.session_state.folder_depth = folder_depth
            
            # Visualization options
            viz_type = st.selectbox(
                "Visualization Type",
                ["Sunburst Chart", "Treemap", "Folder Bar Chart"]
            )
            
            # Metric selection
            metric = st.selectbox(
                "Metric to Display",
                ["Folder Size", "File Count", "Average File Size"]
            )
            
            # Color scheme
            color_scheme = st.selectbox(
                "Color Scheme",
                ["Viridis", "Blues", "Reds", "Greens", "YlOrRd", "Turbo"]
            )
        
        with col2:
            # Convert folder sizes based on selected unit
            folder_hierarchy['size_converted'] = folder_hierarchy['folder_size'].apply(
                lambda x: convert_size(x, size_unit)
            )
            
            # Prepare path columns based on selected depth
            path_columns = [f'Level {i+1}' for i in range(min(folder_depth, max_depth))]
            
            # Configure visualization based on metric
            if metric == "Folder Size":
                value_column = "size_converted"
                title_suffix = f"Size ({size_unit})"
                color_column = "size_converted"
                hover_data = ["file_count", "folder_depth"]
            elif metric == "File Count":
                value_column = "file_count"
                title_suffix = "File Count"
                color_column = "file_count"
                hover_data = ["size_converted", "folder_depth"]
            else:  # Average File Size
                value_column = "avg_file_size"
                title_suffix = f"Avg File Size ({size_unit})"
                color_column = "avg_file_size"
                hover_data = ["file_count", "folder_depth"]
            
            # Create visualization
            try:
                if viz_type == "Sunburst Chart":
                    fig = px.sunburst(
                        folder_hierarchy,
                        path=path_columns,
                        values=value_column,
                        color=color_column,
                        color_continuous_scale=color_scheme.lower(),
                        title=f"Folder Structure by {title_suffix}",
                        hover_data=hover_data,
                        branchvalues="total"
                    )
                elif viz_type == "Treemap":
                    fig = px.treemap(
                        folder_hierarchy,
                        path=path_columns,
                        values=value_column,
                        color=color_column,
                        color_continuous_scale=color_scheme.lower(),
                        title=f"Folder Structure by {title_suffix}",
                        hover_data=hover_data,
                        branchvalues="total"
                    )
                else:  # Folder Bar Chart
                    # Get top folders by selected metric
                    if value_column == "size_converted":
                        sort_column = "folder_size"
                    elif value_column == "file_count":
                        sort_column = "file_count"
                    else:
                        sort_column = "avg_file_size"
                    
                    top_folders = folder_hierarchy.sort_values(by=sort_column, ascending=False).head(20)
                    
                    # Extract last folder name for display
                    top_folders['display_name'] = top_folders['parentPath'].apply(
                        lambda x: x.split('/')[-1] if x and '/' in x else x
                    )
                    
                    fig = px.bar(
                        top_folders,
                        x=value_column,
                        y='display_name',
                        orientation='h',
                        color=color_column,
                        color_continuous_scale=color_scheme.lower(),
                        title=f"Top 20 Folders by {title_suffix}",
                        hover_data=['parentPath'] + hover_data,
                        height=600
                    )
                    fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                
                fig.update_layout(margin=dict(t=50, l=25, r=25, b=25))
                st.plotly_chart(fig, use_container_width=True)
            
            except Exception as e:
                st.error(f"Error creating visualization: {str(e)}")
                st.info("Try adjusting the folder depth or changing the visualization type.")
        
        # Folder Statistics
        st.subheader("ðŸ“Š Folder Statistics")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Size Statistics**")
            total_size = folder_hierarchy['folder_size'].sum()
            st.write(f"Total Size: {convert_size(total_size, size_unit):.2f} {size_unit}")
            
            largest_folder = folder_hierarchy.loc[folder_hierarchy['folder_size'].idxmax()]
            st.write(f"Largest Folder: {largest_folder['parentPath']} ({convert_size(largest_folder['folder_size'], size_unit):.2f} {size_unit})")
            
            deepest_folders = folder_hierarchy[folder_hierarchy['folder_depth'] == folder_hierarchy['folder_depth'].max()]
            st.write(f"Deepest Folder Level: {folder_hierarchy['folder_depth'].max()}")
            
            st.write(f"Average Folder Size: {convert_size(folder_hierarchy['folder_size'].mean(), size_unit):.2f} {size_unit}")
        
        with col2:
            st.write("**File Statistics**")
            total_files = df.shape[0]
            st.write(f"Total Files: {total_files:,}")
            
            avg_files_per_folder = folder_hierarchy['file_count'].mean()
            st.write(f"Average Files per Folder: {avg_files_per_folder:.2f}")
            
            most_files_folder = folder_hierarchy.loc[folder_hierarchy['file_count'].idxmax()]
            st.write(f"Folder with Most Files: {most_files_folder['parentPath']} ({most_files_folder['file_count']} files)")
            
            empty_folders = (folder_hierarchy['file_count'] == 0).sum()
            st.write(f"Empty Folders: {empty_folders}")
        
        # Folder Depth Distribution
        st.subheader("ðŸ“ Folder Depth Distribution")
        depth_counts = folder_hierarchy['folder_depth'].value_counts().sort_index()
        
        depth_fig = px.bar(
            x=depth_counts.index,
            y=depth_counts.values,
            labels={'x': 'Folder Depth', 'y': 'Number of Folders'},
            title="Distribution of Folder Depths",
            color=depth_counts.index,
            color_continuous_scale=color_scheme.lower()
        )
        
        # Add vertical line at average depth
        avg_depth = folder_hierarchy['folder_depth'].mean()
        depth_fig.add_vline(
            x=avg_depth,
            line_dash="dash",
            line_color="red",
            annotation_text=f"Avg: {avg_depth:.1f}",
            annotation_position="top right"
        )
        
        st.plotly_chart(depth_fig, use_container_width=True)
    
    # All other tabs...
    # (Code for other tabs omitted for brevity, but would be included in the full implementation)
    
    # Access Patterns tab
    elif selected_tab == "Access Patterns" and "accessTime" in df.columns:
        render_access_patterns_tab(df)
        
        # Add toggle for detailed stale files analysis
        show_stale_dashboard = st.checkbox("Show Detailed Stale Files Dashboard", value=False)
        
        if show_stale_dashboard:
            render_stale_files_dashboard(df)
        
        # Check if modifyTime is also available
        if "modifyTime" in df.columns:
            # Add toggle for access vs modification comparison
            show_access_mod_compare = st.checkbox("Show Access vs. Modification Comparison", value=False)
            
            if show_access_mod_compare:
                render_access_modification_comparison(df)

else:
    # Show instructions when no file is uploaded
    st.markdown("""
    ## Instructions
    
    Upload your file data CSV or JSON using the uploader in the sidebar.
    
    ### Required Columns:
    
    * `name`: File name
    * `parentPath`: Full path to the parent folder
    * `size`: File size in bytes
    
    ### Optional Columns for Enhanced Features:
    
    * `createTime`: File creation time
    * `modifyTime`: File last modification time  
    * `accessTime`: File last access time *(new)*
    * `classifications`: File classifications or tags
    * `isDeleted`: Whether the file is deleted (1=deleted, 0=active)
    * `dupKey`: Duplicate identification key (e.g., MD5/SHA hash)
    
    ### JSON Format Support
    
    The application now supports JSON file format in addition to CSV. Your JSON should be either:
    
    1. An array of file objects with the above properties
    2. An object with a `files`, `records`, or `data` property containing the array

    ### Example Data Format (JSON):
    ```json
    [
      {
        "name": "report.pdf", 
        "parentPath": "/documents/work", 
        "size": 1048576,
        "createTime": "2023-05-15T09:30:00Z",
        "modifyTime": "2023-05-16T10:15:00Z",
        "accessTime": "2023-06-10T08:45:00Z",
        "classifications": "confidential",
        "isDeleted": 0,
        "dupKey": "a1b2c3d4"
      },
      {
        "name": "presentation.pptx", 
        "parentPath": "/documents/presentations", 
        "size": 2097152,
        "createTime": "2023-06-12T14:45:00Z",
        "modifyTime": "2023-06-12T18:30:00Z",
        "accessTime": "2023-06-15T09:20:00Z",
        "classifications": "internal",
        "isDeleted": 0,
        "dupKey": "e5f6g7h8"
      }
    ]
    ```
    """)